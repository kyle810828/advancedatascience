---
title: "Finalproject"
author: "Kyle"
date: "10/1/2017"
output: html_document
---
## Project choice: Option 3
Perform an analysis of "data scientist" jobs listed on job boards and on the employment pages of major companies. 
1. What are the most common skills that employers look for? 
2. What are the most unique skills that employers look for?
3. Where are the types of companies that employ the most data scientists?

######Load package
```{r,warning=FALSE}
library(rvest)
library(xml2)
library(stringr)
library(plyr)
library(httr)
library(dplyr)
library(rebus)
library(knitr)

```

### Scraping data from GlassDoor and Indeed.
I used rvest package to scrape raw data from job posting website for job title, company industry and website link.
To save the file, I write a .csv file to store them (For example, glassdoor_datascientist_Info.csv)
Besecause the position on the job posting website would change from time to time. Data are unique when they are scraped, and this might cause biased. We have to be careful when we make conclusion.

The following code is to scrap data from GlassDoor with job title 'data scientist'
I scraped 30 pages in the website in this case. 

```{r, eval = FALSE}
#URL of data scientist form glassdoor page 1 to 30

url <- paste0("https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP",c(1:30),".htm")
BasicInfo<-matrix(NA,ncol = 6)

tryCatch({ 
for(i in 1:length(url)){
  
  link<- url[i] %>% read_html() %>% 
    html_nodes("a.jobLink") %>% html_attr("href")
  joblink <- paste0('https://www.glassdoor.com', link)
  joblink <- unique(joblink)
  
tryCatch({  
  for(j in 1:length(joblink)){
    
    #Location
    loc<-joblink[j]%>%read_html() %>% 
      html_nodes(".subtle") %>%  html_text()  %>% str_split(",")
    ##number of char, start with 5
    location <- substr(gsub(" ", "", loc[1], fixed = TRUE), 4, nchar(loc[1])) %>% str_split(", ")
       
    #Load source code
    sourcecode<-suppressWarnings(readLines(joblink[j]))
    
    #title
    title <- sourcecode[str_detect(sourcecode, "\'jobTitle\'")] %>% str_extract("\"(.*)\"") %>% 
      str_replace_all("[[:punct:]]", '')
    title <-title[1]
    
    #company
    company <- sourcecode[str_detect(sourcecode, "\'name\'")] %>% str_extract("\"(.*)\"") %>% 
      str_replace_all("[[:punct:]]", '')
    company <- ifelse(identical(company, character(0)),NA,company)
   
    #industry 
    industry <- sourcecode[str_detect(sourcecode, "\'sector\'")] %>% str_extract("\"(.*)\"") %>% 
      str_replace_all("[[:punct:]]", '')
    industry <- ifelse(identical(industry, character(0)),NA,industry)
    
    #city
    city<-location[[1]][1]
    
    #state
    state<-location[[1]][2]
    
    #state
    website<-joblink[j]
      
    CombineInfo<-cbind(title,company,industry,city,state,website)
    BasicInfo<-rbind(BasicInfo,CombineInfo)
    Sys.sleep(00.1)
  }
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})

  Sys.sleep(00.1)
}
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})

##Handling Error value
goodId<-grepl("^[[:upper:]]+", BasicInfo$city)
Data<-BasicInfo[goodId,]
#write.csv(Data,"glassdoor_datascientist_Info.csv")
```

The following code is to scrap data from Indeed with job title 'data scientist'
I scraped 30 pages in the website in this case. 
```{r,eval=FALSE}
###Find job count

page <- html_session(paste0("https://www.indeed.com/jobs?q=data+scientist&l="))
job_count <- unlist(strsplit(page %>%  read_html() %>%
                               html_node("#searchCount") %>%
                               html_text(), split = ' ')) 
job_count <- as.numeric(str_replace_all(job_count[length(job_count)-1],',',''))
jb=ceiling(as.numeric(job_count)/50)
BasicInfo<-matrix(NA,ncol = 5) 

session <- paste("https://www.indeed.com/jobs?q=data+scientist&l=&radius=25&start="
                 ,seq(0,jb)*50,sep = "")


for(i in 1:length(session)){
  
  title<-session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>%
    html_node("a")%>% html_attr("title")
  
  company<-session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>%
    html_node("span a")%>% html_attr("href")
  
  location<-session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>%
    html_node("span.location") %>% html_text
  
  joblink<- session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>% 
    html_node("h2 a")%>% html_attr("href")
  
  #clean data
  link_clean<-joblink[which(is.na(joblink)==FALSE)]
  link_clean<-paste0("http://indeed.com",link_clean)
  title_clean<-title[which(is.na(joblink)==FALSE)]
  company_clean<-company[which(is.na(joblink)==FALSE)]
  location_clean<-location[which(is.na(joblink)==FALSE)] %>% str_split(",")
  city<-c();state<-c()
  for(k in 1:length(location_clean)){
    city[k]<-location_clean[[k]][1]
    state[k]<-location_clean[[k]][2]%>% str_split(" ")
    state[k]<-state[k][[1]][2]
  }
  
  company_clean<-str_replace_all(company_clean, "[\r\n|\n|\t|\r|,|/|<|>|\\.]|cmp", '')
  
  CombineInfo<-cbind(title_clean,company_clean,city,state,link_clean)
  BasicInfo<-rbind(BasicInfo,CombineInfo)
  Sys.sleep(00.1)
}

#Clean data, delete NA and which does not have company informaiton
BasicInfo<-as.data.frame(BasicInfo)
FinaIndeed<-BasicInfo[-which(duplicated(BasicInfo)),]
FinaIndeed[FinaIndeed[,"company_clean"]=="#","company_clean"]<-NA
FinaIndeed$title_clean<-as.character(FinaIndeed$title_clean)
FinaIndeed$company_clean<-as.character(FinaIndeed$company_clean)
FinaIndeed$city<-as.character(FinaIndeed$city)
FinaIndeed$state<-as.character(FinaIndeed$state)
FinaIndeed$link_clean<-as.character(FinaIndeed$link_clean)

write.csv(FinaIndeed,"datascientist_indeed_info.csv")
```

### Write a Funciton to scrape skill from job link.
In order to identify the skills that employer is looking for, I bulid a keywords set, which contains relevant skills for data scientist. However, this method could only extract some, not all, skills from one webpage, since we define that skill set by ourself. As new requirement comes out, I can expand that skill set in the future if I want to investigate other skill is important or not.


```{r,eval=FALSE}

#used to clean text
##Function need to clean text
  clean.text <- function(text)
  {
    str_replace_all(text, regex('\r\n|\n|\t|\r|,|/|<|>|\\.'), ' ')
  }


#Scraping Skill
ScrSkill<-function(InfoMatrix){
  InfoMatrix<-as.data.frame(InfoMatrix)
  joblink<-as.character(InfoMatrix$website)
  
  #Skill I want to scrape
  keywords <- c('Statistics','Computer science','Math','Machine Learning',
                'Data mining','Statistical Nodeling','Predictive Modeling',
                '\\bR\\b','\\bJava\\b','\\bPython\\b','\\bSQL\\b',
                '\\bSAS\\b','\\bTableau\\b','\\bExcel\\b','\\bC++\\b',
                '\\bHadoop\\b','\\bStata\\b','\\bSpark\\b', '\\bMatlab\\b')
  
  skill_f<-as.data.frame(matrix(NA,ncol = length(keywords)+1))
  colnames(skill_f)<-c("website",sapply(keywords, function(x) gsub("[\\]b", "", x)))

  tryCatch({
    
  pb <- txtProgressBar(min = 1, max = length(joblink), style = 3)
  for(i in 1:length(joblink)){
    
    html <- read_html(joblink[i])
    text <- html_text(html)
    text <- clean.text(text)
    keyskill_logic = unlist(sapply(keywords, function(x) any(grepl(x, text, ignore.case = TRUE,perl = T))))
    names(keyskill_logic) = sapply(keywords, function(x) gsub("[\\]b", "", x))
    skill<-as.data.frame(t(keyskill_logic))
    skill<-cbind(SEQN=joblink[i],skill)
    skill_f<-rbind(skill_f,skill)
    setTxtProgressBar(pb, i)
    
  } 
  close(pb) 
  
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  
  SkillSet<-skill_f[2:nrow(skill_f),]
  return(SkillSet)
}


```

### Scraping skill from job link.
By inputting the the data I scraped from job posting website, which contian joblink. I can create a new data set to see which skill is important.

```{r,eval=FALSE}
GlassdoorInfo<-read.csv("glassdoor_datascientist_Info.csv")
IndeedInfo<-read.csv("datascientist_indeed_info.csv")
GlassdoorSkill<-ScrSkill(GlassdoorInfo)
IndeedSkill<-ScrSkill(IndeedInfo)
#website unique
nrow(GlassdoorSkill)==length(unique(GlassdoorSkill$website))

GlassdoorInfo<-as.data.frame(GlassdoorInfo)
IndeedInfo<-as.data.frame(IndeedInfo)
Glassdoor<-merge(GlassdoorInfo,GlassdoorSkill,by="website")
Indeed<-merge(IndeedInfo,IndeedSkill,by="website")

#sorting data
Glassdoor<-Glassdoor[,2:25]
Glassdoor_final<-Glassdoor[-which(duplicated(Glassdoor)),]
Indeed<-Indeed[,3:25]
Indeed_final<-Indeed[-which(duplicated(Indeed)),]
write.csv(Glassdoor_final,"Glassdoor.csv")
write.csv(Indeed_final,"Indeed.csv")

```

######Output looks like as following:

```{r,include=FALSE}
GlassDoor<-read.csv("GlassDoor.csv")
kable(GlassDoor, format = "html")
